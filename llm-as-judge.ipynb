{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae18e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-4.1-mini-2025-04-14\",api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af90f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class ResponseSignature(dspy.Signature):\n",
    "    context: str = dspy.InputField(\n",
    "        description=\"Context for the task. This should be a clear and concise description of the task to be performed by the model.\"\n",
    "    )\n",
    "    problem_statement: str = dspy.InputField(\n",
    "        description=\"The problem statement provided in the PR. This should be a clear and concise description of the issue being addressed by the PR.\"\n",
    "    )\n",
    "    hints_text: str = dspy.InputField(\n",
    "        description=\"Hints text related to the PR. Can be empty as well. If empty do not flag the PR as problematic.\"\n",
    "    )\n",
    "\n",
    "    problem_category: Literal[\"\"] = dspy.OutputField(\n",
    "        description=\"Category of the problem statement..\"\n",
    "    )\n",
    "\n",
    "    explanation: str = dspy.OutputField(\n",
    "        description=\"Explanation for the decision if the PR is flagged as problematic,Should be CONCISE. If not flagged, this should be an empty string. \"\n",
    "    )\n",
    "    flagged: bool = dspy.OutputField(\n",
    "        description=\"Flag indicating whether the PR is problematic if there is a contradiction between the problem statement and the hints_text. If there is a contradiction, this should be True; otherwise, it should be False.\"\n",
    "    )\n",
    "\n",
    "context = \"\"\"\n",
    "Your task is to determine whether the **problem statement provided in a GitHub pull request (PR)** is consistent with the **hints text**.\n",
    "Specifically, check if any hints text contradicts or corrects the original problem statement. For example, the PR description might say the bug is in file X,\n",
    "but a hint might clarify that the real issue is in file Y. In such cases, you should flag the PR as problematic.\n",
    "Provide an explanation for your decision IF you flag the input.\n",
    "\n",
    "If the problem statement is consistent and unambiguous based on the hints text, return an EMPTY STRING and DO NOT flag the input.\n",
    "\"\"\"\n",
    "\n",
    "predict = dspy.Predict(ResponseSignature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(path=\"princeton-nlp/SWE-bench\", split=\"dev\")\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predict(\n",
    "    context=context,\n",
    "    problem_statement=df[\"problem_statement\"][15],\n",
    "    hints_text=df[\"hints_text\"][15]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def analyze_responses(df, output_file=\"analysis_results.json\",n = 20):\n",
    "    \"\"\"\n",
    "    input: DataFrame with columns 'problem_statement' and 'hints_text' and max n rows to analyze.\n",
    "    output: JSON file with analysis results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i, row in df.iterrows(): \n",
    "        if i >= n:\n",
    "            break\n",
    "        response = predict(\n",
    "            context=context,\n",
    "            problem_statement=row[\"problem_statement\"],\n",
    "            hints_text=row[\"hints_text\"]\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"flagged\": response.flagged,\n",
    "            \"explanation\": response.explanation\n",
    "        })\n",
    "\n",
    "    # Overwrite the file (reset) with new results\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb94a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_responses(df, output_file=\"analysis_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlprojects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
